{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28271ce-27ea-4f98-833c-df5d71d78d74",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "import pprint as pp \n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import ArrayType, StructType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1055333e-e995-465f-be9a-a6ebe20e8450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def set_base_volume_paths():\n",
    "    # VOLUME_PATHS = json.loads(dbutils.notebook.run(\"/Workspace/Users/rishak1997@gmail.com/google-fit-ETL-and-analytics/land_s3_data_to_volume_and_extract\", timeout_seconds= 120))\n",
    "    VOLUME_PATHS = json.loads(dbutils.jobs.taskValues.get(taskKey= \"land_s3_data_to_volume_and_extract\", key = \"output_paths_of_data\"))\n",
    "    return VOLUME_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9605c6ed-e400-4ea7-abea-bf94708193ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set base paths"
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_PATHS = set_base_volume_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd01b352-cfaf-4a3e-9e5d-85bd5b49fb01",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "base config dict"
    }
   },
   "outputs": [],
   "source": [
    "base_config_dict = {\n",
    "    \"activities\": {\n",
    "        \"file_type\": \"xml\",\n",
    "        \"table_name\": \"google_fit.bronze.activities\",\n",
    "        \"vol_paths\" : []\n",
    "    },\n",
    "    \"all sessions\": {\n",
    "        \"file_type\": \"json\",\n",
    "        \"table_name\": \"google_fit.bronze.all_sessions\",\n",
    "        \"vol_paths\" : []\n",
    "    },\n",
    "    \"daily activity metrics\": {\n",
    "        \"file_type\": \"csv\",\n",
    "        \"table_name\": \"google_fit.bronze.daily_activity_metrics\",\n",
    "        \"vol_paths\" : []\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fee80ce-2bcd-460d-a4f0-ac4d029c79aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_bronze_ingestion_config_dict():\n",
    "    \"\"\"\n",
    "    builds and returns the bronze ingestion dict based on the basic config dict with the corresponding volume paths\n",
    "    \"\"\"\n",
    "    bronze_ingestion_config = {k:\n",
    "         v for k, v in base_config_dict.items()}\n",
    "    for path in VOLUME_PATHS:     \n",
    "        for key, value in base_config_dict.items():\n",
    "            if(key in path.lower()):\n",
    "                bronze_ingestion_config[key]['vol_paths'].append(path)\n",
    "    return bronze_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d6600b-2dda-44e4-a867-cf719a13a4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_ingestion_config_dict = build_bronze_ingestion_config_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339aba2e-8ca6-41cc-b601-23dbef110ee0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Print bronze ingestion config dict"
    }
   },
   "outputs": [],
   "source": [
    "pp.pprint(bronze_ingestion_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa78923-c6a3-4629-a57a-16838266b230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be629f5-3fc8-4893-8d53-3bef9bf32a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flatten_df(df):\n",
    "    counter = 0\n",
    "    processed_cols = []\n",
    "    for field_info in df.schema.fields:\n",
    "        if(isinstance(field_info.dataType, ArrayType)):\n",
    "            df = df.withColumn(field_info.name + \"_exploded\", F.explode_outer(F.col(field_info.name)))\n",
    "            processed_cols.append(field_info.name)\n",
    "        elif(isinstance(field_info.dataType, StructType)):\n",
    "            for finfo in field_info.dataType.fields:\n",
    "                df = df.withColumn(f\"{field_info.name}_{finfo.name}\", F.col(f\"{field_info.name}.{finfo.name}\"))\n",
    "                processed_cols.append(field_info.name)\n",
    "        else:\n",
    "            counter += 1\n",
    "            continue\n",
    "    if(counter == len(df.columns)):\n",
    "        return df\n",
    "    else:\n",
    "        df = df.drop(*processed_cols)\n",
    "        return flatten_df(df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc68e90f-0c49-4691-8601-e475867ec007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_for_composite_keys(input_df):\n",
    "    final_list = []\n",
    "\n",
    "    for i in range(0, len(input_df.columns)):\n",
    "        for j in range(0, len(input_df.columns)):\n",
    "            if(len(input_df.columns[i:j])):\n",
    "                col_grp_list =  input_df.columns[i:j]\n",
    "                final_list.append(col_grp_list)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    final_list.sort(key=len)\n",
    "\n",
    "    for cols in final_list:\n",
    "        df = input_df.groupBy(*cols).agg(F.count(F.lit(1)).alias('grouped_count')).select('grouped_count').agg(F.min('grouped_count'), F.max('grouped_count'))    \n",
    "        if(df.collect()[0][0] == 1 and df.collect()[0][1] == 1):\n",
    "            print(\"Found unique combination of columns : \", cols)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db11733-2328-4f74-a3cc-23fbb4a5b567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def all_sessions_ingestion(bronze_ingestion_config_dict):\n",
    "  \"\"\"\n",
    "   Composite key for upsert operation -> ['segment_exploded_endTime', 'segment_exploded_startTime', 'aggregate_exploded_metricName', 'fitnessActivity', 'entity']\n",
    "  \"\"\"\n",
    "  for k, v in bronze_ingestion_config_dict.items():\n",
    "        if(re.search(r\"all\\s*sessions\", k.lower())):\n",
    "            load_path = bronze_ingestion_config_dict[k]['vol_paths']\n",
    "            file_format = bronze_ingestion_config_dict[k][\"file_type\"]\n",
    "            table_name = bronze_ingestion_config_dict[k][\"table_name\"]\n",
    "            print(f\"load_path:  {load_path}\")\n",
    "            print(f\"file_format:  {file_format}\")\n",
    "            print(f\"table_name:  {table_name}\" )\n",
    "            print()    \n",
    "            \n",
    "  df = (\n",
    "    spark.read.format(file_format)\n",
    "    .option(\"multiline\", \"True\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(load_path)\n",
    "    .withColumn(\"aggregate_exploded\", F.explode(\"aggregate\"))\n",
    "    .drop(\"aggregate\")\n",
    "    .withColumn(\"aggregate_exploded_floatValue\", F.col(\"aggregate_exploded.floatValue\"))\n",
    "    .withColumn(\"aggregate_exploded_intValue\", F.col(\"aggregate_exploded.intValue\"))\n",
    "    .withColumn(\"aggregate_exploded_metricName\", F.col(\"aggregate_exploded.metricName\"))\n",
    "    .drop(\"aggregate_exploded\")\n",
    "    .withColumn(\"segment_exploded\", F.explode(\"segment\"))\n",
    "    .drop(\"segment\")\n",
    "    .withColumn(\"segment_exploded_endTime\", F.col(\"segment_exploded.endTime\"))\n",
    "    .withColumn(\"segment_exploded_fitnessActivity\", F.col(\"segment_exploded.fitnessActivity\"))\n",
    "    .withColumn(\"segment_exploded_startTime\", F.col(\"segment_exploded.startTime\"))\n",
    "    .drop('segment_exploded')\n",
    "    .withColumn(\"file_path\", F.col(\"_metadata.file_path\"))\n",
    "    .withColumn(\"etl_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"entity\", F.regexp_extract(F.col('file_path'), r\"[/]takeout-.*_(.*)[\\/]extracted\", 1))\n",
    "    .withColumn(\"file_path\", F.replace(F.replace(F.col('file_path'), F.lit(\"%20\"), F.lit(\" \")), F.lit(\"dbfs:\"), F.lit(\"\")))\n",
    ")\n",
    "  # return df\n",
    "  try:\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "      print(f\"Starting upsert operation on the table :{table_name}\")\n",
    "      (\n",
    "      DeltaTable.forName(spark, table_name).alias(\"t\")\n",
    "        .merge(\n",
    "          df.alias(\"s\"),\n",
    "          \"\"\"\n",
    "          t.entity = s.entity \n",
    "          AND \n",
    "          t.segment_exploded_endTime = s.segment_exploded_endTime \n",
    "          AND\n",
    "          t.segment_exploded_startTime = s.segment_exploded_startTime \n",
    "          AND\n",
    "          t.aggregate_exploded_metricName = s.aggregate_exploded_metricName \n",
    "          AND\n",
    "          t.fitnessActivity = s.fitnessActivity \n",
    "          \"\"\"\n",
    "      ).whenNotMatchedInsertAll()\n",
    "      # .whenMatchedUpdateAll()\n",
    "      .execute()\n",
    "      )\n",
    "      print(f\"Upsert operation was successful on the table :{table_name}\")\n",
    "    else:\n",
    "      print(f\"Starting overwrite operation on the table :{table_name}\")\n",
    "      df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "      print(f\"Overwrite operation was successful on the table :{table_name}\")\n",
    "  except Exception as e:\n",
    "     print(f\"Some error occured while ingesting the data for the table {table_name}. Error details: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b789b530-8845-4247-acad-69c09f858efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def daily_activity_metrics_ingestion(bronze_ingestion_config_dict):\n",
    "  \"\"\"\n",
    "  Composite key for upsert operation -> ['Date', 'entity']\n",
    "  \"\"\"\n",
    "  for k, v in bronze_ingestion_config_dict.items():\n",
    "        if(re.search(r\"daily\\s*activity\\s*metrics\", k.lower())):\n",
    "            load_path = bronze_ingestion_config_dict[k]['vol_paths']\n",
    "            file_format = bronze_ingestion_config_dict[k][\"file_type\"]\n",
    "            table_name = bronze_ingestion_config_dict[k][\"table_name\"]\n",
    "            print(f\"load_path:  {load_path}\")\n",
    "            print(f\"file_format:  {file_format}\")\n",
    "            print(f\"table_name:  {table_name}\")\n",
    "            print()\n",
    "  df = (\n",
    "      spark.read.format(file_format)\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .load(load_path)\n",
    "      .withColumn(\"file_path\", F.col(\"_metadata.file_path\"))\n",
    "      .withColumn(\"etl_timestamp\", F.current_timestamp())\n",
    "      .withColumn(\"entity\", F.regexp_extract(F.col('file_path'), r\"[/]takeout-.*_(.*)[\\/]extracted\", 1))\n",
    "      .withColumn(\"file_path\", F.replace(F.replace(F.col('file_path'), F.lit(\"%20\"), F.lit(\" \")), F.lit(\"dbfs:\"), F.lit(\"\")))\n",
    "  )\n",
    "  df = df.withColumn('date_type', F.when(F.col('Date').rlike(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\"), F.lit('Date_only')).otherwise(F.lit('other')))\n",
    "  df = df.filter(F.col('date_type') != 'other')\n",
    "  df = df.drop('date_type')\n",
    "  clean_cols = [re.sub(r\"\\W+\", \"_\", c) for c in df.columns]\n",
    "  df = df.toDF(*clean_cols)\n",
    "  # return df\n",
    "  try:\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "      print(f\"Starting upsert operation on the table :{table_name}\")\n",
    "      (\n",
    "      DeltaTable.forName(spark, table_name).alias(\"t\")\n",
    "        .merge(\n",
    "          df.alias(\"s\"),\n",
    "          \"t.entity = s.entity AND t.Date = s.Date\"\n",
    "      ).whenNotMatchedInsertAll()\n",
    "      # .whenMatchedUpdateAll()\n",
    "      .execute()\n",
    "      )\n",
    "      print(f\"Upsert operation was successful on the table :{table_name}\")\n",
    "    else:\n",
    "      print(f\"Starting overwrite operation on the table :{table_name}\")\n",
    "      df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "      print(f\"Overwrite operation was successful on the table :{table_name}\")\n",
    "  except Exception as e:\n",
    "     print(f\"Some error occured while ingesting the data for the table {table_name}. Error details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b3e830-04f9-4d02-8979-e04624c81cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def activities_ingestion(bronze_ingestion_config_dict): \n",
    "    \"\"\"\n",
    "    Composite key for upsert operation -> \n",
    "    ['Activity_exploded_Id',\n",
    "    'Activity_exploded__Sport',\n",
    "    'Activity_exploded_Lap_Calories',\n",
    "    'Activity_exploded_Lap_DistanceMeters',\n",
    "    'Activity_exploded_Lap_Intensity',\n",
    "    'Activity_exploded_Lap_TotalTimeSeconds',\n",
    "    'Activity_exploded_Lap_TriggerMethod',\n",
    "    'Activity_exploded_Lap__StartTime',\n",
    "    'Activity_exploded_Lap_Track_Trackpoint_exploded_DistanceMeters',\n",
    "    'Activity_exploded_Lap_Track_Trackpoint_exploded_Time',\n",
    "    'entity' ]\n",
    "    \"\"\"\n",
    "    for k, v in bronze_ingestion_config_dict.items():\n",
    "        if(re.search(r\"activities\", k.lower())):\n",
    "            load_path = bronze_ingestion_config_dict[k]['vol_paths']\n",
    "            file_format = bronze_ingestion_config_dict[k][\"file_type\"]\n",
    "            table_name = bronze_ingestion_config_dict[k][\"table_name\"]\n",
    "            print(f\"load_path:  {load_path}\")\n",
    "            print(f\"file_format:  {file_format}\")\n",
    "            print(f\"table_name:  {table_name}\")\n",
    "            print()\n",
    "    df = (\n",
    "        spark.read.format(file_format)\n",
    "        .option(\"rootTag\", \"Activity\")\n",
    "        .option(\"rowTag\", \"Activities\")\n",
    "        .option(\"inferSchema\", True)\n",
    "        .option(\"treatEmptyValuesAsNulls\", \"true\") \n",
    "        .load(load_path)\n",
    "        \n",
    "    )\n",
    "    df = flatten_df(df)\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"file_path\", F.col(\"_metadata.file_path\"))\n",
    "        .withColumn(\"etl_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"entity\", F.regexp_extract(F.col('file_path'), r\"[/]takeout-.*_(.*)[\\/]extracted\", 1))\n",
    "        .withColumn(\"file_path\", F.replace(F.replace(F.col('file_path'), F.lit(\"%20\"), F.lit(\" \")), F.lit(\"dbfs:\"), F.lit(\"\")))\n",
    "    )\n",
    "    try:\n",
    "        if spark.catalog.tableExists(table_name):\n",
    "            print(f\"Starting upsert operation on the table :{table_name}\")\n",
    "            (\n",
    "            DeltaTable.forName(spark, table_name).alias(\"t\")\n",
    "                .merge(\n",
    "                df.alias(\"s\"),\n",
    "                \"\"\"\n",
    "                t.entity = s.entity \n",
    "                AND\n",
    "                t.Activity_exploded_Id = s.Activity_exploded_Id \n",
    "                AND \n",
    "                t.Activity_exploded__Sport = s.Activity_exploded__Sport \n",
    "                AND \n",
    "                t.Activity_exploded_Lap_Calories = s.Activity_exploded_Lap_Calories \n",
    "                AND  \n",
    "                t.Activity_exploded_Lap_DistanceMeters = s.Activity_exploded_Lap_DistanceMeters \n",
    "                AND\n",
    "                t.Activity_exploded_Lap_Intensity = s.Activity_exploded_Lap_Intensity \n",
    "                AND\n",
    "                t.Activity_exploded_Lap_TotalTimeSeconds = s.Activity_exploded_Lap_TotalTimeSeconds \n",
    "                AND\n",
    "                t.Activity_exploded_Lap_TriggerMethod = s.Activity_exploded_Lap_TriggerMethod \n",
    "                AND\n",
    "                t.Activity_exploded_Lap__StartTime = s.Activity_exploded_Lap__StartTime \n",
    "                AND\n",
    "                t.Activity_exploded_Lap_Track_Trackpoint_exploded_DistanceMeters = s.Activity_exploded_Lap_Track_Trackpoint_exploded_DistanceMeters \n",
    "                AND\n",
    "                t.Activity_exploded_Lap_Track_Trackpoint_exploded_Time = s.Activity_exploded_Lap_Track_Trackpoint_exploded_Time \n",
    "                \"\"\"\n",
    "            ).whenNotMatchedInsertAll()\n",
    "            # .whenMatchedUpdateAll()\n",
    "            .execute()\n",
    "            )\n",
    "            print(f\"Upsert operation was successful on the table :{table_name}\")\n",
    "        else:\n",
    "            print(f\"Starting overwrite operation on the table :{table_name}\")\n",
    "            df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "            print(f\"Overwrite operation was successful on the table :{table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Some error occured while ingesting the data for the table {table_name}. Error details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed382f80-0a8d-41ed-83f7-be1b46ca5c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bronze_ingestion(bronze_ingestion_config_dict):\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(len(bronze_ingestion_config_dict)) as e:\n",
    "       all_sessions_ingestion_future  = e.submit(all_sessions_ingestion, bronze_ingestion_config_dict)\n",
    "       futures.append(all_sessions_ingestion_future)\n",
    "       daily_activity_metrics_ingestion_future  = e.submit(daily_activity_metrics_ingestion, bronze_ingestion_config_dict)\n",
    "       futures.append(daily_activity_metrics_ingestion_future)\n",
    "       activities_ingestion_future  = e.submit(activities_ingestion, bronze_ingestion_config_dict)\n",
    "       futures.append(activities_ingestion_future)\n",
    "    for f in as_completed(futures):\n",
    "        f.result()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f06d8497-120f-4f97-8bd1-c84d97fa4d98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main call"
    }
   },
   "outputs": [],
   "source": [
    "bronze_ingestion(bronze_ingestion_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d582f72-7836-469d-8ebb-111db293d71d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(\"bronze_max_ingestion_timestamps_dict\", json.dumps(\n",
    "    {v['table_name'] : spark.table(v['table_name']).agg(F.max(F.col('etl_timestamp')).cast(StringType())).collect()[0][0] for _, v in bronze_ingestion_config_dict.items()}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b64de4-75e0-465c-8f54-4cec5a14ab28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exit notebook"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(json.dumps(\n",
    "    {v['table_name'] : spark.table(v['table_name']).agg(F.max(F.col('etl_timestamp')).cast(StringType())).collect()[0][0] for _, v in bronze_ingestion_config_dict.items()}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d32e91-d940-4330-ac9b-0b3523cd386f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook exited: {\"google_fit.bronze.activities\": \"2025-09-12 11:03:56.009982\", \"google_fit.bronze.all_sessions\": \"2025-09-12 11:03:59.785936\", \"google_fit.bronze.daily_activity_metrics\": \"2025-09-12 11:02:53.288683\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee1a173-8553-435b-abea-fcd9998b519d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.sql(\" show tables in google_fit.bronze\").selectExpr(\"concat_ws('.', database, tableName) as table\")\n",
    "\n",
    "# tables = [t['table'] for t in df.collect()]\n",
    "\n",
    "# for t in tables:\n",
    "#     spark.sql(f\"drop table google_fit.{t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d381c59-d0a9-47f6-b09d-57d59c1cf241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def all_data_ingestion(bronze_ingestion_dict):\n",
    "#     for k, v in bronze_ingestion_dict.items():\n",
    "#         if(re.search(r\"all\\s*data\", k.lower())):\n",
    "#             load_path = k\n",
    "#             file_format = bronze_ingestion_dict[k][\"file_type\"]\n",
    "#             table_name = bronze_ingestion_dict[k][\"table_name\"]\n",
    "#             print(f\"load_path:  {load_path}\")\n",
    "#             print(f\"file_format:  {file_format}\")\n",
    "#             print(f\"table_name:  {table_name}\")\n",
    "\n",
    "#     df = (\n",
    "#     spark.read.format(file_format)\n",
    "#     .option(\"inferSchema\", \"true\")\n",
    "#     .load(\"/Volumes/google_fit/bronze/landing_zone/takeout-20250906T131146Z-1-001_20250907173921/extracted/Takeout/Fit/All data/derived_com.google.calories.bmr_com.google.and.json\")\n",
    "#     .withColumn(\"fitValue_exploded\", F.explode(\"fitValue\"))\n",
    "#     .drop(\"fitValue\")\n",
    "#     .withColumn(\"fitValue_exploded_value\", F.col(\"fitValue_exploded.value\"))\n",
    "#     .drop(\"fitValue_exploded\")\n",
    "#     .withColumn(\"fitValue_exploded_value_intVal\", F.col(\"fitValue_exploded_value.intVal\"))\n",
    "#     .drop(\"fitValue_exploded_value\")\n",
    "# )\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e654539-2a95-4503-a534-f36854239540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(all_data_ingestion(bronze_ingestion_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1e79864-e44b-4f15-ab1b-32c5f17987be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# nested_df = (\n",
    "#      spark.read.format(\"xml\")\n",
    "#         .option(\"rootTag\", \"Activity\")\n",
    "#         .option(\"rowTag\", \"Activities\")\n",
    "#         .option(\"inferSchema\", True)\n",
    "#         .option(\"treatEmptyValuesAsNulls\", \"true\") \n",
    "#         .load(\"/Volumes/google_fit/bronze/landing_zone/takeout-20250906T131146Z-1-001_20250907173921_rishabh/extracted/Takeout/Fit/Activities/\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "824cd76d-8e9d-4b30-935d-40d7b1927430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for field in nested_df.schema.fields:\n",
    "#         print(field.name)\n",
    "#         print(field.dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496d760e-13e0-408c-8c7b-2ef87f1f6480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check = flatten_df(nested_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34da9ed6-f24b-4391-9075-80de6f47d1a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(check\n",
    "#         .groupBy(*check.columns)\n",
    "#         .count()\n",
    "#         .orderBy(F.desc(\"count\"))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba01e3fd-cdb7-4c18-82ae-87e0cfebeaab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7abcb768-eff7-4c3b-b46c-051f60ce5c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check_for_composite_keys(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053ec225-e81e-47b0-98c2-5828e60b6d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d780ad-06d7-42b2-921a-2b07bd2e1489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check.count() == check.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9487508-27dc-424b-98f6-b9b7198a982f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def activities_ingestion(bronze_ingestion_config_dict): \n",
    "#     for k, v in bronze_ingestion_config_dict.items():\n",
    "#         if(re.search(r\"activities\", k.lower())):\n",
    "#             load_path = bronze_ingestion_config_dict[k]['vol_paths']\n",
    "#             file_format = bronze_ingestion_config_dict[k][\"file_type\"]\n",
    "#             table_name = bronze_ingestion_config_dict[k][\"table_name\"]\n",
    "#             print(f\"load_path:  {load_path}\")\n",
    "#             print(f\"file_format:  {file_format}\")\n",
    "#             print(f\"table_name:  {table_name}\")\n",
    "#             print()\n",
    "#     df = (\n",
    "#         spark.read.format(file_format)\n",
    "#         .option(\"rootTag\", \"Activity\")\n",
    "#         .option(\"rowTag\", \"Activities\")\n",
    "#         .option(\"inferSchema\", True)\n",
    "#         .option(\"treatEmptyValuesAsNulls\", \"true\") \n",
    "#         .load(*load_path)\n",
    "#         .withColumn(\"Activity_Id\", F.col(\"Activity.Id\"))\n",
    "#         .withColumn(\"Activity_Lap\", F.col(\"Activity.Lap\"))\n",
    "#         .withColumn(\"Activity_Sport\", F.col(\"Activity._Sport\"))\n",
    "#         .drop('Activity')\n",
    "#         .withColumn(\"Activity_Lap_Calories\", F.col('Activity_Lap.Calories'))\n",
    "#         .withColumn(\"Activity_Lap_DistanceMeters\", F.col('Activity_Lap.DistanceMeters'))\n",
    "#         .withColumn(\"Activity_Lap_Intensity\", F.col('Activity_Lap.Intensity'))\n",
    "#         .withColumn(\"Activity_Lap_TotalTimeSeconds\", F.col('Activity_Lap.TotalTimeSeconds'))\n",
    "#         .withColumn(\"Activity_Lap_Track\", F.col('Activity_Lap.Track'))\n",
    "#         .withColumn(\"Activity_Lap_TriggerMethod\", F.col('Activity_Lap.TriggerMethod'))\n",
    "#         .withColumn(\"Activity_Lap_StartTime\", F.col('Activity_Lap._StartTime'))\n",
    "#         .drop(\"Activity_Lap\")\n",
    "#         .withColumn(\"Activity_Lap_Track_Trackpoint\", F.col('Activity_Lap_Track.Trackpoint'))\n",
    "#         .drop(\"Activity_Lap_Track\")\n",
    "#         .withColumn(\"Activity_Lap_Track_Trackpoint_exploded\", F.explode('Activity_Lap_Track_Trackpoint'))\n",
    "#         .drop(\"Activity_Lap_Track_Trackpoint\")\n",
    "#         .withColumn(\"Activity_Lap_Track_Trackpoint_exploded_DistanceMeters\", F.col('Activity_Lap_Track_Trackpoint_exploded.DistanceMeters'))\n",
    "#         .withColumn(\"Activity_Lap_Track_Trackpoint_exploded_Time\", F.col('Activity_Lap_Track_Trackpoint_exploded.Time'))\n",
    "#         .drop(\"Activity_Lap_Track_Trackpoint_exploded\")\n",
    "#     )\n",
    "#     for field in df.schema.fields:\n",
    "#         if isinstance(field.dataType, ArrayType):\n",
    "#             df = df.withColumn(f\"{field.name}_exploded\", F.explode(F.col(field.name)))\n",
    "#             df = df.drop(field.name)\n",
    "#     return (\n",
    "#         df.dropDuplicates()\n",
    "#         .withColumn(\"file_path\", F.col(\"_metadata.file_path\"))\n",
    "#         .withColumn(\"etl_timestamp\", F.current_timestamp())\n",
    "#         .withColumn(\"entity\", F.regexp_extract(F.col('file_path'), r\"[/]takeout-.*_(.*)[\\/]extracted\", 1))\n",
    "#         .withColumn(\"file_path\", F.replace(F.replace(F.col('file_path'), F.lit(\"%20\"), F.lit(\" \")), F.lit(\"dbfs:\"), F.lit(\"\")))\n",
    "#     )\n",
    "#     try:\n",
    "#         if spark.catalog.tableExists(table_name):\n",
    "#             print(f\"Starting upsert operation on the table :{table_name}\")\n",
    "#             (\n",
    "#             DeltaTable.forName(spark, table_name).alias(\"t\")\n",
    "#                 .merge(\n",
    "#                 df.alias(\"s\"),\n",
    "#                 \"t.entity = s.entity AND t.Activity_exploded_Id = s.Activity_exploded_Id\"\n",
    "#             ).whenNotMatchedInsertAll()\n",
    "#             .whenMatchedUpdateAll()\n",
    "#             .execute()\n",
    "#             )\n",
    "#             print(f\"Upsert operation was successful on the table :{table_name}\")\n",
    "#         else:\n",
    "#             print(f\"Starting overwrite operation on the table :{table_name}\")\n",
    "#             df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "#             print(f\"Overwrite operation was successful on the table :{table_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Some error occured while ingesting the data for the table {table_name}. Error details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ed4666-39a1-45d1-b9b6-c7d6c0bffad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(\n",
    "#     daily_activity_metrics_ingestion(bronze_ingestion_config_dict)\n",
    "#         .withColumn('date_type', F.when(F.col('Date').rlike(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\"), F.lit('Date_only')).otherwise(F.lit('other')))\n",
    "#         .filter(F.col('date_type') == 'other')\n",
    "#         .groupBy('Date').count()\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8350458137554329,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
