{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4379c223-180c-4e65-a2d3-80c6e6c7b6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.window import Window as W\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pyspark.sql.connect.dataframe import DataFrame\n",
    "import copy\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b277af9-4ef9-4723-974d-aae74e234a28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CreateDF class"
    }
   },
   "outputs": [],
   "source": [
    "class CreateDF:\n",
    "    __base_catalog = \"google_fit\"\n",
    "    __base_schema = \"silver\"\n",
    "    __base_table = \"daily_activity_metrics\"\n",
    "    __base_table_name = f\"{__base_catalog}.{__base_schema}.{__base_table}\"\n",
    "    __excepted_cols = [\"etl_timestamp\", \"file_path\"]\n",
    "\n",
    "    @classmethod\n",
    "    def get_tables(cls, schema_name: str):\n",
    "        return [\n",
    "            row[\"table\"]\n",
    "            for row in spark.sql(f\"SHOW TABLES IN {cls.__base_catalog}.{schema_name}\")\n",
    "            .selectExpr(\n",
    "                f\"concat_ws('.', '{cls.__base_catalog}', database, tableName) as table\"\n",
    "            )\n",
    "            .filter(\"\"\" table not rlike \"_sqldf\" \"\"\")\n",
    "            .collect()\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def from_table(cls, schema_name: str = None, table_name: str = None):\n",
    "        if schema_name is None and table_name is None:\n",
    "            ref_table = spark.table(cls.__base_table_name)\n",
    "        elif table_name not in cls.get_tables(schema_name) == 0:\n",
    "            raise Exception(\"Given table or schema does not exist!\")\n",
    "        else:\n",
    "            try:\n",
    "                ref_table = spark.table(\n",
    "                    f\"{cls.__base_catalog}.{schema_name}.{table_name}\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Some error occured when reading the referenced table into a DataFrame: {e}\"\n",
    "                )\n",
    "        cols = ref_table.columns\n",
    "        return ref_table.select(\n",
    "            *[col for col in cols if col not in cls.__excepted_cols]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86aab2d6-8af3-463a-84df-138560e46c94",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Base DeclarativeAggregations Class"
    }
   },
   "outputs": [],
   "source": [
    "class DeclarativeAggregations:\n",
    "    __basic_agg_config = {\n",
    "        \"count\": [],\n",
    "        \"avg\": [],\n",
    "        \"sum\": [],\n",
    "        \"max\": [],\n",
    "        \"min\": [],\n",
    "        \"lag\": [],\n",
    "        \"rank\": [],\n",
    "        \"first_value\": [],\n",
    "    }\n",
    "\n",
    "    def __init__(self, df: DataFrame):\n",
    "        if isinstance(df, DataFrame):\n",
    "            self.df = df\n",
    "            self.df_trans = None\n",
    "            self.agg_config = copy.deepcopy(self.__class__.__basic_agg_config)\n",
    "            self.entities = None\n",
    "        else:\n",
    "            raise Exception(\"df is not a Spark DataFrame\")\n",
    "\n",
    "    def define_entities(self, entities: str | list[str] = None):\n",
    "        if entities is None:\n",
    "            self.entities = [\n",
    "                row[\"entity\"] for row in self.df.select(\"entity\").distinct().collect()\n",
    "            ]\n",
    "            return\n",
    "        if isinstance(entities, str):\n",
    "            entities_list = [entities]\n",
    "        else:\n",
    "            entities_list = entities\n",
    "        if set(entities_list).issubset(\n",
    "            {row[\"entity\"] for row in self.df.select(\"entity\").distinct().collect()}\n",
    "        ):\n",
    "            self.entities = list(set(entities_list))\n",
    "        else:\n",
    "            raise Exception(\"Given entity does not exist in the dataframe!\")\n",
    "\n",
    "    @staticmethod\n",
    "    def cols_checker(df: DataFrame, cols: str | list[str]) -> bool:\n",
    "        check = lambda cols, df: (\n",
    "            set(cols).issubset(set(df.columns))\n",
    "            if type(cols) == list\n",
    "            else set([cols]).issubset(set(df.columns))\n",
    "        )\n",
    "        return check(cols, df)\n",
    "\n",
    "    def build_agg_config(\n",
    "        self,\n",
    "        agg_metric: str,\n",
    "        group_by_cols: str | list[str],\n",
    "        name: str,\n",
    "        agg_on_col: str = None,\n",
    "        offset: int = 1,\n",
    "        default=None,\n",
    "        order_by: str = None,\n",
    "        rows_between_args: tuple[int, int] = (None, None),\n",
    "    ):\n",
    "        if agg_metric not in self.agg_config.keys():\n",
    "            raise Exception(\"Given aggregation type is not supported!\")\n",
    "        if self.df_trans is None:\n",
    "            if not DeclarativeAggregations.cols_checker(self.df, group_by_cols):\n",
    "                raise Exception(\n",
    "                    \"Given partition columns do not exist in the dataframe!\"\n",
    "                )\n",
    "            if agg_metric not in [\"rank\"]:\n",
    "                if not DeclarativeAggregations.cols_checker(self.df, agg_on_col):\n",
    "                    raise Exception(\"Given agg_on does not exist in the dataframe!\")\n",
    "        else:\n",
    "            if not (DeclarativeAggregations.cols_checker(self.df_trans, group_by_cols)):\n",
    "                raise Exception(\n",
    "                    \"Given partition columns do not exist in the trans dataframe!\"\n",
    "                )\n",
    "            if agg_metric not in [\"rank\"]:\n",
    "                if not DeclarativeAggregations.cols_checker(self.df_trans, agg_on_col):\n",
    "                    raise Exception(\n",
    "                        \"Given agg_on does not exist in the trans dataframe!\"\n",
    "                    )\n",
    "\n",
    "        config = {\"group_by_cols\": group_by_cols, \"name\": name}\n",
    "        if agg_metric in [\"rank\"]:\n",
    "            if not order_by:\n",
    "                raise Exception(\"Order by is required for rank!\")\n",
    "            config[\"order_by\"] = order_by\n",
    "        elif agg_metric in [\"lag\"]:\n",
    "            if not order_by:\n",
    "                raise Exception(\"Order by is required for lag!\")\n",
    "            config[\"offset\"] = offset\n",
    "            config[\"default\"] = default\n",
    "            config[\"order_by\"] = order_by\n",
    "            config[\"agg_on_col\"] = agg_on_col\n",
    "        elif agg_metric in [\"first_value\"]:\n",
    "            if None in rows_between_args:\n",
    "                raise Exception(\"Rows between args is required for first_value!\")\n",
    "            config[\"rows_between_args\"] = rows_between_args\n",
    "        config[\"agg_on_col\"] = agg_on_col\n",
    "        self.agg_config[agg_metric].append(config)\n",
    "\n",
    "    def build_trans_df(self):\n",
    "        def get_expr_for_agg_metric(agg_metric, derived_col_info):\n",
    "            if agg_metric == \"avg\":\n",
    "                return F.avg(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "            elif agg_metric == \"max\":\n",
    "                return F.max(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "            elif agg_metric == \"min\":\n",
    "                return F.min(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "            elif agg_metric == \"sum\":\n",
    "                return F.sum(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "            elif agg_metric == \"count\":\n",
    "                return F.count(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "            elif agg_metric == \"lag\":\n",
    "                return F.lag(\n",
    "                    F.col(derived_col_info[\"agg_on_col\"]),\n",
    "                    derived_col_info.get(\"offset\", 1),\n",
    "                    derived_col_info.get(\"default\"),\n",
    "                )\n",
    "            elif agg_metric == \"rank\":\n",
    "                return F.dense_rank()\n",
    "            elif agg_metric == \"first_value\":\n",
    "                return F.first_value(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "\n",
    "        if not self.df_trans:\n",
    "            self.df_trans = self.df.filter(F.col(\"entity\").isin(self.entities))\n",
    "        for agg_metric in self.agg_config.keys():\n",
    "            for derived_col_info in self.agg_config[agg_metric]:\n",
    "                w_spec = W.partitionBy(derived_col_info[\"group_by_cols\"])\n",
    "                if agg_metric in [\"lag\", \"rank\"] and derived_col_info.get(\"order_by\"):\n",
    "                    w_spec = w_spec.orderBy(derived_col_info[\"order_by\"])\n",
    "                if agg_metric in [\"first_value\"]:\n",
    "                    w_spec = w_spec.rowsBetween(*derived_col_info[\"rows_between_args\"])\n",
    "                expr = get_expr_for_agg_metric(agg_metric, derived_col_info)\n",
    "                self.df_trans = self.df_trans.withColumn(\n",
    "                    derived_col_info[\"name\"], expr.over(w_spec)\n",
    "                )\n",
    "\n",
    "    def add_comparison_col_percent(\n",
    "        self, prev_col: str, curr_col: str, comp_col_name: str\n",
    "    ):\n",
    "        if self.df_trans is None:\n",
    "            raise Exception(\"Trans dataframe is not built yet!\")\n",
    "        if (\n",
    "            prev_col not in self.df_trans.columns\n",
    "            or curr_col not in self.df_trans.columns\n",
    "        ):\n",
    "            raise Exception(\"Given col does not exist in the trans dataframe!\")\n",
    "        schema_trans = self.df_trans.schema\n",
    "        if schema_trans[prev_col].dataType != schema_trans[curr_col].dataType:\n",
    "            raise Exception(\"Given cols are not of the same type!\")\n",
    "        self.df_trans = self.df_trans.withColumn(\n",
    "            comp_col_name,\n",
    "            F.when(\n",
    "                F.col(prev_col) != F.lit(0),\n",
    "                F.round((F.col(curr_col) - F.col(prev_col)) * 100 / F.col(prev_col), 3),\n",
    "            ).otherwise(F.lit(0)),\n",
    "        )\n",
    "\n",
    "    def clear_agg_config(self):\n",
    "        self.agg_config = copy.deepcopy(self.__class__.__basic_agg_config)\n",
    "\n",
    "    def clear_df_trans(self):\n",
    "        self.df_trans = None\n",
    "\n",
    "    def current_attributes(self):\n",
    "        return self.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8aea5ae-0830-4b33-a999-c9b5003f23aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LinePlot class"
    }
   },
   "outputs": [],
   "source": [
    "class LinePlot:\n",
    "    __base_fig_size = (25, 15)\n",
    "\n",
    "    __axis_is_a_comparison_col = lambda y: True if re.search(r\"percent\", y) else None\n",
    "    __get_regex_for_prev_col = lambda y: (\n",
    "        False\n",
    "        if re.search(r\"_(.+)_change\", y) is None\n",
    "        else re.search(r\"_(.+)_change\", y).group(1)\n",
    "    )\n",
    "    __map_percent_col_to_previous_col = lambda regex, col: (\n",
    "        False if re.search(rf\"(prev.*{regex})\", col) is None else col\n",
    "    )\n",
    "\n",
    "    __add_ly = lambda match: match.group(1) + \"ly_\"\n",
    "    __change_y_name_if_not_percent_col = lambda y: (\n",
    "        re.sub(r\"^([a-z]+)_\", LinePlot.__add_ly, y.replace(\"current_\", \"\"))\n",
    "        if \"percent\" not in y\n",
    "        else y\n",
    "    )\n",
    "\n",
    "    __base_save_path = \"/Volumes/google_fit/gold/activity_metrics_plots/\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        DeclarativeAggregations_obj: DeclarativeAggregations,\n",
    "        x,\n",
    "        y,\n",
    "        fig_size: tuple[int, int] = None,\n",
    "        save_path: str = None,\n",
    "    ):\n",
    "        if not isinstance(DeclarativeAggregations_obj, DeclarativeAggregations):\n",
    "            raise Exception(\"Given object is not a DeclarativeAggregations object!\")\n",
    "        if x not in DeclarativeAggregations_obj.df_trans.columns:\n",
    "            raise Exception(\"Given x column does not exist in the dataframe!\")\n",
    "        if y not in DeclarativeAggregations_obj.df_trans.columns:\n",
    "            raise Exception(\"Given y column does not exist in the dataframe!\")\n",
    "\n",
    "        if LinePlot.__axis_is_a_comparison_col(y):\n",
    "            regex = LinePlot.__get_regex_for_prev_col(y)\n",
    "            for col in DeclarativeAggregations_obj.df_trans.columns:\n",
    "                prev_col = LinePlot.__map_percent_col_to_previous_col(regex, col)\n",
    "                if prev_col:\n",
    "                    break\n",
    "            self.excluded_dates_for_comparison_col = (\n",
    "                LinePlot.__find_inconsistent_datapoints(\n",
    "                    DeclarativeAggregations_obj.df_trans, x, y\n",
    "                )\n",
    "            )\n",
    "            temp = DeclarativeAggregations_obj.df_trans\n",
    "            temp = temp.filter(F.col(prev_col).isNotNull())\n",
    "            for entity in self.excluded_dates_for_comparison_col.keys():\n",
    "                temp = temp.filter(\n",
    "                    ~(\n",
    "                        (F.col(\"entity\") == entity)\n",
    "                        & (\n",
    "                            F.col(x).isin(\n",
    "                                self.excluded_dates_for_comparison_col[entity]\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            self.df_pd = temp.select([x, y, \"entity\"]).toPandas()\n",
    "        else:\n",
    "            self.df_pd = DeclarativeAggregations_obj.df_trans.select(\n",
    "                [x, y, \"entity\"]\n",
    "            ).toPandas()\n",
    "        self.df_pd.rename(\n",
    "            columns={y: LinePlot.__change_y_name_if_not_percent_col(y)}, inplace=True\n",
    "        )\n",
    "        self.x = x\n",
    "        self.y = LinePlot.__change_y_name_if_not_percent_col(y)\n",
    "        self.save_path = (\n",
    "            save_path\n",
    "            if save_path is not None\n",
    "            else f\"{LinePlot.__base_save_path}/{self.y}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.png\"\n",
    "        )\n",
    "        self.fig_size = fig_size if fig_size is not None else LinePlot.__base_fig_size\n",
    "\n",
    "    @staticmethod\n",
    "    def __find_inconsistent_datapoints(df_trans, x, y):\n",
    "        datapoints = dict()\n",
    "        entities = [e[\"entity\"] for e in df_trans.select(\"entity\").distinct().collect()]\n",
    "        for entity in entities:\n",
    "            datapoints[entity] = list()\n",
    "        # prev_cols = [col for col in df_trans.columns if re.search(r\"prev_\", col)]\n",
    "        # if not len(prev_cols):\n",
    "        #     raise ValueError(\"Generate prev columns first\")\n",
    "        # date_col = 'week' if 'week' in prev_cols[0] else 'month'\n",
    "        for entity in entities:\n",
    "            datapoints[entity].extend(\n",
    "                [\n",
    "                    date[x].strftime(\"%Y-%m-%d\")\n",
    "                    for date in df_trans.filter(F.col(\"entity\") == entity)\n",
    "                    .groupBy(\"entity\", x)\n",
    "                    .agg(F.countDistinct(y).alias(\"count\"))\n",
    "                    .filter(F.col(\"count\") > 1)\n",
    "                    .select(x)\n",
    "                    .collect()\n",
    "                ]\n",
    "            )\n",
    "        return {k: list(set(v)) for k, v in datapoints.items()}\n",
    "\n",
    "    def plot(self, fig_size: tuple[int, int] = None):\n",
    "        fig, ax = plt.subplots(figsize=self.fig_size)\n",
    "        sns.lineplot(x=self.x, y=self.y, data=self.df_pd, ax=ax, hue=\"entity\")\n",
    "        plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_layer_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
