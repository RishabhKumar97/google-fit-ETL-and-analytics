{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4379c223-180c-4e65-a2d3-80c6e6c7b6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.window import Window as W\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pyspark.sql.connect.dataframe import DataFrame\n",
    "import copy\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b277af9-4ef9-4723-974d-aae74e234a28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CreateDF class"
    }
   },
   "outputs": [],
   "source": [
    "class CreateDF:\n",
    "    __base_catalog = \"google_fit\"\n",
    "    __base_schema = \"silver\"\n",
    "    __base_table = \"daily_activity_metrics\"\n",
    "    __base_table_name = f\"{__base_catalog}.{__base_schema}.{__base_table}\"\n",
    "    __excepted_cols = ['etl_timestamp', 'file_path']\n",
    "\n",
    "    @classmethod\n",
    "    def get_tables(cls, schema_name: str):\n",
    "        return [\n",
    "            row[\"table\"]\n",
    "            for row in spark.sql(\n",
    "                f\"SHOW TABLES IN {cls.__base_catalog}.{schema_name}\"\n",
    "            ).selectExpr(\n",
    "                f\"concat_ws('.', '{cls.__base_catalog}', database, tableName) as table\"\n",
    "            ).filter(\"\"\" table not rlike \"_sqldf\" \"\"\")\n",
    "            .collect()\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def from_table(cls, schema_name: str= None, table_name:str = None):\n",
    "        if schema_name is None and table_name is None:\n",
    "            ref_table = spark.table(cls.__base_table_name)\n",
    "        elif(table_name not in cls.get_tables(schema_name) == 0):\n",
    "            raise Exception(\"Given table or schema does not exist!\")\n",
    "        else:\n",
    "            try:\n",
    "                ref_table = spark.table(f\"{cls.__base_catalog}.{schema_name}.{table_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Some error occured when reading the referenced table into a DataFrame: {e}\")\n",
    "        cols = ref_table.columns\n",
    "        return ref_table.select(*[col for col in cols if col not in cls.__excepted_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86aab2d6-8af3-463a-84df-138560e46c94",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Base DeclarativeAggregations Class"
    }
   },
   "outputs": [],
   "source": [
    "class DeclarativeAggregations:\n",
    "    __basic_agg_config = {\n",
    "        \"count\": [],\n",
    "        \"avg\": [],\n",
    "        \"sum\": [],\n",
    "        \"max\": [],\n",
    "        \"min\": [],\n",
    "        \"lag\": [],\n",
    "        \"rank\": []\n",
    "    }\n",
    "     \n",
    "    def __init__(self, df: DataFrame):\n",
    "        if(isinstance(df, DataFrame)):\n",
    "            self.df = df\n",
    "            self.df_trans = None\n",
    "            self.agg_config =  copy.deepcopy(self.__class__.__basic_agg_config)\n",
    "            self.entities = None\n",
    "        else:\n",
    "            raise Exception(\"df is not a Spark DataFrame\")\n",
    "\n",
    "    def define_entities(self, entities: str|list[str] = None):\n",
    "        if(entities is None):\n",
    "            self.entities = [row['entity'] for row in self.df.select('entity').distinct().collect()]\n",
    "            return\n",
    "        if isinstance(entities, str):\n",
    "            entities_list = [entities]\n",
    "        else:\n",
    "            entities_list = entities\n",
    "        if(set(entities_list).issubset({row['entity'] for row in self.df.select('entity').distinct().collect()})):\n",
    "            self.entities = list(set(entities_list))\n",
    "        else:\n",
    "            raise Exception(\"Given entity does not exist in the dataframe!\")\n",
    "\n",
    "    @staticmethod\n",
    "    def cols_checker(df: DataFrame, cols: str|list[str]) -> bool:\n",
    "        check = lambda cols, df: set(cols).issubset(set(df.columns)) if type(cols) == list else set([cols]).issubset(set(df.columns))\n",
    "        return check(cols, df)\n",
    "    \n",
    "    def build_agg_config(\n",
    "        self, agg_metric: str, group_by_cols: str | list[str], name: str, \n",
    "        agg_on_col: str= None, offset: int = 1, default=None, order_by: str = None\n",
    "    ):\n",
    "        if agg_metric not in self.agg_config.keys():\n",
    "            raise Exception(\"Given aggregation type is not supported!\")\n",
    "        if self.df_trans is None:\n",
    "            if not DeclarativeAggregations.cols_checker(self.df, group_by_cols):\n",
    "                raise Exception(\"Given partition columns do not exist in the dataframe!\")\n",
    "            if agg_metric not in [\"rank\"]:\n",
    "                if not DeclarativeAggregations.cols_checker(self.df, agg_on_col):\n",
    "                    raise Exception(\"Given agg_on does not exist in the dataframe!\")\n",
    "        else:\n",
    "            if not (DeclarativeAggregations.cols_checker(self.df_trans, group_by_cols)):\n",
    "                raise Exception(\"Given partition columns do not exist in the trans dataframe!\")\n",
    "            if agg_metric not in [\"rank\"]:\n",
    "                if not DeclarativeAggregations.cols_checker(self.df_trans, agg_on_col):\n",
    "                    raise Exception(\"Given agg_on does not exist in the trans dataframe!\")\n",
    "\n",
    "        config = {\n",
    "            \"group_by_cols\": group_by_cols,\n",
    "            \"name\": name\n",
    "        }\n",
    "        if agg_metric in [\"rank\"]:\n",
    "            if not order_by:\n",
    "                raise Exception(\"Order by is required for rank!\")\n",
    "            config[\"order_by\"] = order_by\n",
    "        else:\n",
    "            if agg_metric in [\"lag\"]:\n",
    "                if not order_by:\n",
    "                    raise Exception(\"Order by is required for lag!\")\n",
    "                config[\"offset\"] = offset\n",
    "                config[\"default\"] = default\n",
    "                config[\"order_by\"] = order_by\n",
    "            config['agg_on_col'] = agg_on_col\n",
    "        self.agg_config[agg_metric].append(config)\n",
    "\n",
    "    \n",
    "    def build_trans_df(self):\n",
    "        def get_expr_for_agg_metric(agg_metric, derived_col_info):\n",
    "            if agg_metric == \"avg\":\n",
    "                return F.avg(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "            elif agg_metric == \"max\":\n",
    "                return F.max(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "            elif agg_metric == \"min\":\n",
    "                return F.min(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "            elif agg_metric == \"sum\":\n",
    "                return F.sum(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "            elif agg_metric == \"count\":\n",
    "                return F.count(F.col(derived_col_info[\"agg_on_col\"]))\n",
    "            elif agg_metric == \"lag\":\n",
    "                return F.lag(\n",
    "                    F.col(derived_col_info[\"agg_on_col\"]),\n",
    "                    derived_col_info.get(\"offset\", 1),\n",
    "                    derived_col_info.get(\"default\")\n",
    "                )\n",
    "            elif agg_metric == \"rank\":\n",
    "                return F.dense_rank()\n",
    " \n",
    "        if not self.df_trans:\n",
    "            self.df_trans = self.df.filter(F.col('entity').isin(self.entities))\n",
    "        for agg_metric in self.agg_config.keys():\n",
    "            for derived_col_info in self.agg_config[agg_metric]:\n",
    "                w_spec = W.partitionBy(derived_col_info['group_by_cols'])\n",
    "                if agg_metric in [\"lag\", \"rank\"] and derived_col_info.get(\"order_by\"):\n",
    "                    w_spec = w_spec.orderBy(derived_col_info[\"order_by\"])\n",
    "                expr = get_expr_for_agg_metric(agg_metric, derived_col_info)\n",
    "                self.df_trans = self.df_trans.withColumn(derived_col_info[\"name\"], expr.over(w_spec))\n",
    "\n",
    "    def add_comparison_col_percent(self, prev_col: str, curr_col: str, comp_col_name: str):\n",
    "        if self.df_trans is None:\n",
    "            raise Exception(\"Trans dataframe is not built yet!\")\n",
    "        if prev_col not in self.df_trans.columns or curr_col not in self.df_trans.columns:\n",
    "            raise Exception(\"Given col does not exist in the trans dataframe!\")\n",
    "        schema_trans = self.df_trans.schema\n",
    "        if schema_trans[prev_col].dataType != schema_trans[curr_col].dataType:\n",
    "            raise Exception(\"Given cols are not of the same type!\")\n",
    "        self.df_trans = self.df_trans.withColumn(comp_col_name, F.when(F.col(prev_col) != F.lit(0), F.round((F.col(curr_col) - F.col(prev_col))*100/F.col(prev_col), 3)).otherwise(F.lit(0)))\n",
    "    \n",
    "    def clear_agg_config(self):\n",
    "        self.agg_config = copy.deepcopy(self.__class__.__basic_agg_config)\n",
    "\n",
    "    def clear_df_trans(self):\n",
    "        self.df_trans = None\n",
    "\n",
    "    def current_attributes(self):\n",
    "        return self.__dict__\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8aea5ae-0830-4b33-a999-c9b5003f23aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LinePlot class"
    }
   },
   "outputs": [],
   "source": [
    "class LinePlot:\n",
    "    __base_fig_size= (25, 15)\n",
    "\n",
    "\n",
    "    __axis_is_a_comparison_col = lambda y: True if re.search(r'percent', y) else None\n",
    "    __get_regex_for_prev_col = lambda y: False if re.search(r\"_(.+)_change\", y) is None else re.search(r\"_(.+)_change\", y).group(1)\n",
    "    __map_percent_col_to_previous_col = lambda regex, col: False if re.search(fr\"(prev.*{regex})\", col) is None else col\n",
    "\n",
    "\n",
    "    __base_save_path = \"/Volumes/google_fit/gold/weekly_plots_activity_metrics\"\n",
    "\n",
    "    def __init__(self, DeclarativeAggregations_obj: DeclarativeAggregations, x, y, fig_size: tuple[int, int]= None, save_path: str= None):\n",
    "        if not isinstance(DeclarativeAggregations_obj, DeclarativeAggregations):\n",
    "            raise Exception(\"Given object is not a DeclarativeAggregations object!\")\n",
    "        if x not in DeclarativeAggregations_obj.df_trans.columns:\n",
    "            raise Exception(\"Given x column does not exist in the dataframe!\")\n",
    "        if y not in DeclarativeAggregations_obj.df_trans.columns:\n",
    "            raise Exception(\"Given y column does not exist in the dataframe!\")\n",
    "        if LinePlot.__axis_is_a_comparison_col(y):\n",
    "            regex = LinePlot.__get_regex_for_prev_col(y)\n",
    "            for col in DeclarativeAggregations_obj.df_trans.columns:\n",
    "                prev_col = LinePlot.__map_percent_col_to_previous_col(regex, col)\n",
    "                if prev_col:\n",
    "                    break\n",
    "            self.df_pd = DeclarativeAggregations_obj.df_trans.filter(F.col(prev_col).isNotNull()).select([x, y, 'entity']).toPandas()\n",
    "        else:\n",
    "            self.df_pd = DeclarativeAggregations_obj.df_trans.select([x, y, 'entity']).toPandas()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.save_path = save_path if save_path is not None else f\"{LinePlot.__base_save_path}/{self.y}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.png\"\n",
    "        self.fig_size = fig_size if fig_size is not None else LinePlot.__base_fig_size\n",
    "\n",
    "    def plot(self, fig_size: tuple[int, int]= None):\n",
    "        fig, ax = plt.subplots(figsize= self.fig_size)\n",
    "        sns.lineplot(x= self.x, y= self.y, data=self.df_pd, ax= ax, hue= 'entity')\n",
    "        plt.tight_layout()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_layer_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
